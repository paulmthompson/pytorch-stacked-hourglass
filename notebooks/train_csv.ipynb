{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba2da0-3f67-41d0-8ec6-2f6944a8c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,copy\n",
    "file_path = '../src/stacked_hourglass'\n",
    "sys.path.append(os.path.dirname(file_path))\n",
    "\n",
    "# https://stackoverflow.com/questions/42212810/tqdm-in-jupyter-notebook-prints-new-progress-bars-repeatedly\n",
    "from tqdm.notebook import trange, tqdm \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import DataParallel\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from stacked_hourglass import hg2\n",
    "from stacked_hourglass.utils.logger import Logger\n",
    "from stacked_hourglass.datasets.csv import CSV\n",
    "from stacked_hourglass.utils.misc import save_checkpoint, adjust_learning_rate\n",
    "from stacked_hourglass.csv_train import do_training_epoch, do_validation_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e37df2-611e-4a93-bff9-4ede0fc8f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '/home/wanglab/Data/image_labels_shuffled.csv'\n",
    "data_folder = '/home/wanglab/Data/Licking_Data'\n",
    "checkpoint = 'checkpoint_csv'\n",
    "input_shape = (256, 256)\n",
    "arch = 'hg2'\n",
    "\n",
    "train_batch = 16 #This works with my 12Gb video card, but probably need 16 batches for 8Gb (This is for HG2)\n",
    "test_batch = 16\n",
    "\n",
    "workers = 2\n",
    "\n",
    "lr = 5e-4\n",
    "\n",
    "start_epoch = 0\n",
    "epochs = 150\n",
    "snapshot = 0\n",
    "\n",
    "best_f1 = 0\n",
    "\n",
    "device = torch.device('cuda', torch.cuda.current_device())\n",
    "\n",
    "# Disable gradient calculations by default.\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# create checkpoint dir\n",
    "os.makedirs(checkpoint, exist_ok=True)\n",
    "\n",
    "if arch == 'hg1':\n",
    "    model = hg1(pretrained=False, input_channels=1, num_classes=1)\n",
    "elif arch == 'hg2':\n",
    "    model = hg2(pretrained=False, input_channels=1, num_classes=1)\n",
    "elif arch == 'hg8':\n",
    "    model = hg8(pretrained=False, input_channels=1, num_classes=1)\n",
    "else:\n",
    "    raise Exception('unrecognised model architecture: ' + arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62adae48-a08d-4838-8f5c-f8f2e65fef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load previous model\n",
    "filename_to_load = 'checkpoint_csv/checkpoint.pth.tar'\n",
    "loaded_checkpoint = torch.load(filename_to_load)\n",
    "\n",
    "state_dict = loaded_checkpoint['state_dict']\n",
    "\n",
    "if sorted(state_dict.keys())[0].startswith('module.'):\n",
    "    model = DataParallel(model)\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512ae4f-4e33-4419-b2c1-6acc23b626b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load and finetune\n",
    "if arch == 'hg1':\n",
    "    model = hg1(pretrained=False)\n",
    "elif arch == 'hg2':\n",
    "    model = hg2(pretrained=False)\n",
    "elif arch == 'hg8':\n",
    "    model = hg8(pretrained=False)\n",
    "else:\n",
    "    raise Exception('unrecognised model architecture: ' + arch)\n",
    "\n",
    "filename_to_load = 'checkpoint_mpii/checkpoint.pth.tar'\n",
    "loaded_checkpoint = torch.load(filename_to_load)\n",
    "\n",
    "state_dict = loaded_checkpoint['state_dict']\n",
    "\n",
    "if sorted(state_dict.keys())[0].startswith('module.'):\n",
    "    model = DataParallel(model)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "#Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.module.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.module.fc_.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "score=[]\n",
    "for i in range(0,len(model.module.score)):\n",
    "    score.append(nn.Conv2d(256, 1, kernel_size=1, bias=True))\n",
    "model.module.score = nn.ModuleList(score)\n",
    "score_ = [nn.Conv2d(1,256,kernel_size=1,bias=True)]\n",
    "model.module.score_ = nn.ModuleList(score_)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if (param.requires_grad == True):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc110cc7-4962-4984-81f0-319b86b2d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DataParallel(model).to(device)\n",
    "\n",
    "train_dataset = CSV(csv_path, data_folder, is_train=True, inp_res=input_shape,input_channels=3)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch, shuffle=True,\n",
    "    num_workers=workers, pin_memory=True\n",
    ")\n",
    "\n",
    "val_dataset = copy.deepcopy(train_dataset)\n",
    "val_dataset.is_train = False\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=test_batch, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True\n",
    ")\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "print('The total size of the training set is ', len(train_loader)*train_batch)\n",
    "print('The total size of the validation set is ', len(val_loader)*test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960fdfe3-3e08-4360-84a5-73c932c65971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and eval\n",
    "start_epoch = 150\n",
    "epochs = 250\n",
    "\n",
    "for epoch in trange(start_epoch, epochs, desc='Overall', ascii=True):\n",
    "\n",
    "    # train for one epoch\n",
    "    train_loss, train_f1, train_PPV, train_sens = do_training_epoch(train_loader, model, device, optimizer)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    valid_loss, predictions, valid_f1, valid_PPV, valid_sens = do_validation_epoch(val_loader, model, device)\n",
    "\n",
    "    # print metrics\n",
    "    tqdm.write(f'[{epoch + 1:3d}/{epochs:3d}] lr={lr:0.2e} '\n",
    "                   f'train_loss={train_loss:0.4f} '\n",
    "                   f'valid_loss={valid_loss:0.4f} ')\n",
    "\n",
    "    writer.add_scalar('Loss/train', train_loss,epoch)\n",
    "    writer.add_scalar('Loss/test', valid_loss,epoch)\n",
    "    writer.add_scalar('F1/train', train_f1,epoch)\n",
    "    writer.add_scalar('F1/test',valid_f1,epoch)\n",
    "    writer.add_scalar('PPV/train',train_PPV,epoch)\n",
    "    writer.add_scalar('PPV/test',valid_PPV,epoch)\n",
    "    writer.add_scalar('Sensitivity/train',train_sens,epoch)\n",
    "    writer.add_scalar('Sensitivity/test',valid_sens,epoch)\n",
    "\n",
    "    # remember best acc and save checkpoint\n",
    "    is_best = valid_f1 > best_f1\n",
    "    best_acc = max(valid_f1, best_f1)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': arch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc': best_f1,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, predictions, is_best, checkpoint=checkpoint, snapshot=snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca37620-cc68-4819-b9d2-b09c1f5343e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "#plt.pcolor(TF.autocontrast(train_dataset[0][0])[0,:,:])\n",
    "plt.pcolor(train_dataset[0][1][0,:,:])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5c38c1-9d77-4808-87ec-64cb83b27888",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[1][1][0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f59f46-68d9-4027-a7f0-df1a86887c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://discuss.pytorch.org/t/computing-the-mean-and-std-of-dataset/34949\n",
    "# !!!I should ONLY do this for training data and not test data !!!\n",
    "\n",
    "mydataset = CSV(csv_path, data_folder, is_train=False, inp_res=input_shape,training_split=0.0)\n",
    "myloader = DataLoader(\n",
    "    mydataset,\n",
    "    batch_size=train_batch, shuffle=True,\n",
    "    num_workers=workers, pin_memory=True\n",
    ")\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "for images, _ in myloader:\n",
    "    batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "\n",
    "mean /= len(myloader.dataset)\n",
    "std /= len(myloader.dataset)\n",
    "\n",
    "print('Mean: ', mean)\n",
    "print('Std: ', std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch-wsl]",
   "language": "python",
   "name": "conda-env-torch-wsl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
